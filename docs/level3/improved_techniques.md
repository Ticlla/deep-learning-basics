# Level 3: Improved Neural Network Techniques

> Go from 94% to 98% accuracy with cross-entropy, regularization, and better initialization.

---

## Table of Contents

1. [Overview](#1-overview)
2. [Cross-Entropy Cost Function](#2-cross-entropy-cost-function)
3. [L2 Regularization](#3-l2-regularization)
4. [Better Weight Initialization](#4-better-weight-initialization)
5. [Putting It All Together](#5-putting-it-all-together)
6. [Code Reference](#6-code-reference)
7. [Exploration Scripts](#7-exploration-scripts)

---

## 1. Overview

Level 2 achieved ~94.5% accuracy. Level 3 improvements get us to ~98%:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEVEL 3 IMPROVEMENTS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  Improvement          Problem Solved           Accuracy Gain     â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  Cross-Entropy        Learning slowdown        +0.5%             â”‚   â”‚
â”‚  â”‚  Regularization       Overfitting              +1.5%             â”‚   â”‚
â”‚  â”‚  Weight Init          Saturation               +0.5%             â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  Total improvement:   94% â†’ 97-98%                               â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Cross-Entropy Cost Function

### 2.1 The Problem: Learning Slowdown

With **quadratic cost** (MSE), the gradient contains Ïƒ'(z):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE LEARNING SLOWDOWN PROBLEM                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Quadratic Cost Gradient:                                               â”‚
â”‚                                                                          â”‚
â”‚   âˆ‚C/âˆ‚w = (a - y) Ã— Ïƒ'(z) Ã— input                                       â”‚
â”‚                      â†‘                                                   â”‚
â”‚                   PROBLEM!                                               â”‚
â”‚                                                                          â”‚
â”‚   When prediction is very wrong (Ïƒ(z) â‰ˆ 0 or 1):                        â”‚
â”‚   â€¢ Ïƒ'(z) â‰ˆ 0  (derivative is tiny!)                                    â”‚
â”‚   â€¢ Gradient becomes tiny                                               â”‚
â”‚   â€¢ Learning SLOWS DOWN exactly when we need it most!                   â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚        Ïƒ'(z)                                                             â”‚
â”‚        0.25 â”¤        â”Œâ”€â”€â”€â”€â”                                             â”‚
â”‚             â”‚       /      \                                             â”‚
â”‚        0.1  â”¤      /        \                                            â”‚
â”‚             â”‚     /          \                                           â”‚
â”‚        0    â”¼â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â–º  z                                 â”‚
â”‚            -6   -2    0    2    6                                        â”‚
â”‚                                                                          â”‚
â”‚            â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’                                          â”‚
â”‚             "Learning dead zone"                                         â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 The Solution: Cross-Entropy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CROSS-ENTROPY COST FUNCTION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Formula:                                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚   C = -(1/n) Ã— Î£ [yÂ·ln(a) + (1-y)Â·ln(1-a)]                              â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Gradient (the magic!):                                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚   âˆ‚C/âˆ‚w = (a - y) Ã— input                                               â”‚
â”‚           â†‘                                                              â”‚
â”‚        NO Ïƒ'(z)!                                                         â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Benefits:                                                              â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚   â€¢ Larger error â†’ Larger gradient â†’ Faster learning                    â”‚
â”‚   â€¢ No Ïƒ'(z) term â†’ No learning slowdown                                â”‚
â”‚   â€¢ Network learns FASTER when predictions are wrong                    â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Comparison (target y=1):                                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚                                                                          â”‚
â”‚   Gradient                                                               â”‚
â”‚        1 â”¤                    â•±â•±â•±â•±â•±  Cross-Entropy                      â”‚
â”‚          â”‚                  â•±â•±                                           â”‚
â”‚     0.5  â”¤              â•±â•±â•±â•±                                             â”‚
â”‚          â”‚          â•±â•±â•±â•±    â”€â”€â”€â”€â”€â”€ Quadratic                            â”‚
â”‚        0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  a                               â”‚
â”‚          0        0.5         1                                          â”‚
â”‚          (wrong)            (correct)                                    â”‚
â”‚                                                                          â”‚
â”‚   When aâ‰ˆ0 (very wrong), cross-entropy gradient is MUCH larger!         â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Code

```python
class CrossEntropyCost:
    @staticmethod
    def fn(a, y):
        """Cost function."""
        return np.sum(np.nan_to_num(-y*np.log(a) - (1-y)*np.log(1-a)))
    
    @staticmethod
    def delta(z, a, y):
        """Output layer error (gradient)."""
        return (a - y)  # â† Simple! No Ïƒ'(z)
```

---

## 3. L2 Regularization

### 3.1 The Problem: Overfitting

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE OVERFITTING PROBLEM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Overfitting = Network memorizes training data                         â”‚
â”‚                 instead of learning general patterns                     â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Accuracy                                                               â”‚
â”‚      100% â”¤    â”â”â”â”â”â”â”â”â”â”â”â”â”  Training (memorized!)                     â”‚
â”‚           â”‚   â•±                                                          â”‚
â”‚       95% â”¤  â•±   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Test (doesn't generalize)                 â”‚
â”‚           â”‚ â•±   â•±                                                        â”‚
â”‚       80% â”¼â•±                                                             â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  Epochs                            â”‚
â”‚                                                                          â”‚
â”‚   Symptoms:                                                              â”‚
â”‚   â€¢ Training accuracy keeps improving                                   â”‚
â”‚   â€¢ Test/validation accuracy plateaus or drops                          â”‚
â”‚   â€¢ Large gap between training and test accuracy                        â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 The Solution: L2 Regularization (Weight Decay)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    L2 REGULARIZATION                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Modified Cost Function:                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚                                                                          â”‚
â”‚   C = Câ‚€ + (Î»/2n) Ã— Î£ wÂ²                                                â”‚
â”‚       â†‘        â†‘                                                         â”‚
â”‚   original  regularization                                               â”‚
â”‚    cost     term (penalty)                                               â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   What it does:                                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚   â€¢ Adds penalty for large weights                                      â”‚
â”‚   â€¢ Forces network to use smaller, distributed weights                  â”‚
â”‚   â€¢ Prevents any single connection from dominating                      â”‚
â”‚   â€¢ Results in smoother, more general solutions                         â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Weight Update (with decay):                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚                                                                          â”‚
â”‚   w_new = (1 - Î·Â·Î»/n) Ã— w_old - Î· Ã— âˆ‚Câ‚€/âˆ‚w                              â”‚
â”‚           â†‘                                                              â”‚
â”‚       "weight decay"                                                     â”‚
â”‚       (weights shrink by factor (1 - Î·Â·Î»/n) each step)                  â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Choosing Î»:                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚   Î» = 0:     No regularization (may overfit)                            â”‚
â”‚   Î» = 0.1:   Light regularization                                       â”‚
â”‚   Î» = 1.0:   Moderate regularization                                    â”‚
â”‚   Î» = 5.0:   Strong regularization (good for MNIST)                     â”‚
â”‚   Î» = 10+:   Very strong (may underfit)                                 â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Visual: With vs Without Regularization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGULARIZATION EFFECT                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   WITHOUT Regularization (Î»=0)        WITH Regularization (Î»=5.0)       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
â”‚                                                                          â”‚
â”‚   Accuracy                            Accuracy                           â”‚
â”‚      100% â”â”â”â”â”â”â” Training           100% â”¤                             â”‚
â”‚       90% â”€â”€â”€â”€â”€â”€â”€ Test                95% â”â”â”â”â”â”â” Training              â”‚
â”‚                                       93% â”€â”€â”€â”€â”€â”€â”€ Test                  â”‚
â”‚                                                                          â”‚
â”‚   Gap: ~10%  â† BAD!                  Gap: ~2%   â† GOOD!                 â”‚
â”‚   (overfitting)                      (generalizing)                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Better Weight Initialization

### 4.1 The Problem: Neuron Saturation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SATURATION PROBLEM                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   With 784 inputs and weights ~ N(0, 1):                                â”‚
â”‚                                                                          â”‚
â”‚   z = Î£(w_i Ã— x_i) + b                                                  â”‚
â”‚       784 terms                                                          â”‚
â”‚                                                                          â”‚
â”‚   By Central Limit Theorem:                                             â”‚
â”‚   â€¢ z has std â‰ˆ âˆš784 â‰ˆ 28                                               â”‚
â”‚   â€¢ |z| is typically 10-30                                              â”‚
â”‚                                                                          â”‚
â”‚   Result:                                                                â”‚
â”‚   â€¢ Ïƒ(z) â‰ˆ 0 or 1 (saturated!)                                          â”‚
â”‚   â€¢ Ïƒ'(z) â‰ˆ 0 (gradient vanishes!)                                      â”‚
â”‚   â€¢ Learning is EXTREMELY slow at start                                 â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Distribution of z values (bad initialization):                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚                                                                          â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚
â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     (almost empty)           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º z                   â”‚
â”‚      -30           0            30                                       â”‚
â”‚        â†‘                         â†‘                                       â”‚
â”‚   Ïƒ(z)=0                    Ïƒ(z)=1                                       â”‚
â”‚   Ïƒ'(z)â‰ˆ0                   Ïƒ'(z)â‰ˆ0                                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 The Solution: 1/âˆšn Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BETTER INITIALIZATION                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   New initialization:                                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚                                                                          â”‚
â”‚   w ~ N(0, 1/âˆšn_in)                                                     â”‚
â”‚                                                                          â”‚
â”‚   For 784 inputs:                                                       â”‚
â”‚   w ~ N(0, 1/âˆš784) = N(0, 1/28) = N(0, 0.036)                          â”‚
â”‚                                                                          â”‚
â”‚   Now:                                                                   â”‚
â”‚   â€¢ z has std â‰ˆ 1 (not 28!)                                             â”‚
â”‚   â€¢ Neurons stay in the "active" region                                 â”‚
â”‚   â€¢ Ïƒ'(z) is reasonable â†’ learning happens!                             â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Distribution of z values (good initialization):                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                                          â”‚
â”‚              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                â”‚
â”‚            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                            â”‚
â”‚          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º z                   â”‚
â”‚         -3     -1    0    1     3                                        â”‚
â”‚                â†‘                                                         â”‚
â”‚           Ïƒ'(z) is larger here!                                          â”‚
â”‚                                                                          â”‚
â”‚                                                                          â”‚
â”‚   Code comparison:                                                       â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚
â”‚                                                                          â”‚
â”‚   # Old (network.py - bad):                                              â”‚
â”‚   weights = np.random.randn(y, x)                                       â”‚
â”‚                                                                          â”‚
â”‚   # New (network2.py - good):                                            â”‚
â”‚   weights = np.random.randn(y, x) / np.sqrt(x)                          â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Putting It All Together

### 5.1 The Complete Improved Network

```python
import network2

# Create network with ALL improvements
net = network2.Network(
    [784, 100, 10],                    # Larger hidden layer
    cost=network2.CrossEntropyCost     # Better cost function
)
# Note: 1/âˆšn initialization is automatic in network2

# Train with regularization
net.SGD(
    training_data, 
    epochs=30, 
    mini_batch_size=10, 
    eta=0.5,
    lmbda=5.0,                         # L2 regularization
    evaluation_data=validation_data,
    monitor_evaluation_accuracy=True
)
```

### 5.2 Results Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ACCURACY COMPARISON                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Configuration                              Test Accuracy               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚                                                                          â”‚
â”‚   Level 2: Basic [784, 30, 10]               ~94.5%                     â”‚
â”‚                                                                          â”‚
â”‚   Level 3: + Cross-Entropy                   ~95%                       â”‚
â”‚           + Regularization                   ~96.5%                     â”‚
â”‚           + Better Init                      ~97%                       â”‚
â”‚           + Larger Hidden [784, 100, 10]     ~97.5-98%                  â”‚
â”‚                                                                          â”‚
â”‚   Level 4: CNN (coming next)                 ~99%+                      â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Code Reference

### 6.1 network2.py Key Differences from network.py

| Feature | network.py | network2.py |
|---------|------------|-------------|
| Cost Function | Quadratic only | Quadratic or Cross-Entropy |
| Regularization | None | L2 (weight decay) |
| Weight Init | N(0, 1) | N(0, 1/âˆšn) |
| Monitoring | Basic | Cost & accuracy tracking |
| Save/Load | No | Yes (JSON) |

### 6.2 Key Functions

```python
# Cost classes
class QuadraticCost:
    def fn(a, y): ...      # Cost value
    def delta(z, a, y): ...  # Gradient (includes Ïƒ')

class CrossEntropyCost:
    def fn(a, y): ...      # Cost value  
    def delta(z, a, y): ...  # Gradient (NO Ïƒ'!)

# Network class
class Network:
    def __init__(sizes, cost=CrossEntropyCost): ...
    def default_weight_initializer(): ...  # 1/âˆšn scaling
    def large_weight_initializer(): ...    # Old N(0,1) for comparison
    def SGD(training_data, epochs, mini_batch_size, eta,
            lmbda=0.0,                     # Regularization strength
            evaluation_data=None,
            monitor_evaluation_cost=False,
            monitor_evaluation_accuracy=False,
            monitor_training_cost=False,
            monitor_training_accuracy=False): ...
```

---

## 7. Exploration Scripts

Run these from the `src/` directory:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LEVEL 3 EXPLORATION SCRIPTS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    src/level3/
    â””â”€â”€ 1_improved_network.py
        â”‚
        â”œâ”€â”€ Section 1: Cross-Entropy Cost
        â”‚   â””â”€â”€ Compare quadratic vs cross-entropy learning
        â”‚
        â”œâ”€â”€ Section 2: L2 Regularization  
        â”‚   â””â”€â”€ Demonstrate overfitting and fix
        â”‚
        â”œâ”€â”€ Section 3: Weight Initialization
        â”‚   â””â”€â”€ Compare old vs new initialization
        â”‚
        â””â”€â”€ Section 4: Full Training
            â””â”€â”€ Train with ALL improvements â†’ ~98% accuracy
    
    Run: python level3/1_improved_network.py
```

### Script Output

Visualizations saved to timestamped folders:
```
pictures/level3/YYYYMMDD_HHMMSS/
â”œâ”€â”€ 1_cross_entropy_advantage.png
â”œâ”€â”€ 2_cost_comparison.png
â”œâ”€â”€ 3_regularization_effect.png
â”œâ”€â”€ 4_initialization_comparison.png
â””â”€â”€ 5_full_training.png
```

---

## Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEVEL 3 KEY TAKEAWAYS                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    âœ“ Cross-Entropy Cost
      â€¢ Gradient = (a - y), no Ïƒ'(z)
      â€¢ Faster learning when predictions are wrong
    
    âœ“ L2 Regularization
      â€¢ C = Câ‚€ + (Î»/2n) Î£ wÂ²
      â€¢ Prevents overfitting by penalizing large weights
      â€¢ Good Î» for MNIST: 5.0
    
    âœ“ Better Initialization
      â€¢ Weights ~ N(0, 1/âˆšn_in)
      â€¢ Keeps neurons in active region
      â€¢ Avoids saturation at start
    
    ğŸ“ˆ Result: 94% â†’ 97-98% accuracy!
    
    
    NEXT: Level 4 - Convolutional Neural Networks
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    python level4/1_convolutional_networks.py
    
    â†’ Get to 99%+ accuracy with CNNs!
```

---

*Documentation for Level 3 of Neural Networks and Deep Learning*


