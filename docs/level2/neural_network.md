# Level 2: Building Your First Neural Network

> Deep dive into how neural networks work: architecture, feedforward, backpropagation, and training.

---

## Table of Contents

1. [Overview](#1-overview)
2. [Network Architecture](#2-network-architecture)
3. [Weights and Biases](#3-weights-and-biases)
4. [Sigmoid Activation](#4-sigmoid-activation)
5. [Feedforward Computation](#5-feedforward-computation)
6. [Cost Function](#6-cost-function)
7. [Backpropagation](#7-backpropagation)
8. [Stochastic Gradient Descent](#8-stochastic-gradient-descent)
9. [Training Process](#9-training-process)
10. [Code Reference](#10-code-reference)
11. [Exercises](#11-exercises)

---

## 1. Overview

A neural network is a computational model inspired by biological neurons. It learns to recognize patterns by adjusting internal parameters (weights and biases) based on examples.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NEURAL NETWORK OVERVIEW                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   INPUT              HIDDEN              OUTPUT                         â”‚
â”‚   (784 pixels)       (30 features)       (10 probabilities)            â”‚
â”‚                                                                         â”‚
â”‚      â—‹                   â—‹                   â—‹  â†’ P(0) = 0.01          â”‚
â”‚      â—‹                   â—‹                   â—‹  â†’ P(1) = 0.02          â”‚
â”‚      â—‹       WÂ¹, bÂ¹      â—‹       WÂ², bÂ²      â—‹  â†’ P(2) = 0.95  â† MAX   â”‚
â”‚     ...    â”€â”€â”€â”€â”€â”€â”€â–º     ...    â”€â”€â”€â”€â”€â”€â”€â–º     ...                        â”‚
â”‚      â—‹                   â—‹                   â—‹  â†’ P(9) = 0.01          â”‚
â”‚                                                                         â”‚
â”‚   "What is           "I see             "It's a 2!"                    â”‚
â”‚    this digit?"       curves..."                                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Learning Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                        â”‚
â”‚  1. FEEDFORWARD        2. COMPUTE ERROR       3. BACKPROPAGATE        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”‚
â”‚  Pass input through    Compare output         Calculate how each      â”‚
â”‚  network to get        to correct answer      weight contributed      â”‚
â”‚  prediction                                   to the error            â”‚
â”‚                                                                        â”‚
â”‚       â—‹â”€â”€â”€â–ºâ—‹â”€â”€â”€â–ºâ—‹            â—‹                    â—‹â—„â”€â”€â”€â—‹â—„â”€â”€â”€â—‹         â”‚
â”‚                          prediction               gradients            â”‚
â”‚                             â†“                                          â”‚
â”‚                         - target                                       â”‚
â”‚                             â†“                                          â”‚
â”‚                           error                                        â”‚
â”‚                                                                        â”‚
â”‚  4. UPDATE WEIGHTS                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  Adjust weights to reduce error:  w_new = w_old - Î· Ã— gradient        â”‚
â”‚                                                                        â”‚
â”‚  5. REPEAT for thousands of examples until network learns!            â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Network Architecture

### Layer Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NETWORK [784, 30, 10]                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  LAYER 0              LAYER 1              LAYER 2                      â”‚
â”‚  INPUT                HIDDEN               OUTPUT                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                                         â”‚
â”‚  784 neurons          30 neurons           10 neurons                   â”‚
â”‚  (no weights)         (learns features)    (predictions)               â”‚
â”‚                                                                         â”‚
â”‚     aâ°                    aÂ¹                   aÂ²                       â”‚
â”‚    (784,1)              (30,1)               (10,1)                     â”‚
â”‚                                                                         â”‚
â”‚           â”€â”€â”€â”€WÂ¹â”€â”€â”€â”€â–º          â”€â”€â”€â”€WÂ²â”€â”€â”€â”€â–º                             â”‚
â”‚           (30Ã—784)             (10Ã—30)                                  â”‚
â”‚                                                                         â”‚
â”‚           â”€â”€â”€â”€bÂ¹â”€â”€â”€â”€â–º          â”€â”€â”€â”€bÂ²â”€â”€â”€â”€â–º                             â”‚
â”‚           (30Ã—1)               (10Ã—1)                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why These Sizes?

| Layer | Neurons | Reason |
|-------|---------|--------|
| Input | 784 | One per pixel (28Ã—28 = 784) |
| Hidden | 30 | Enough to learn features, not too slow |
| Output | 10 | One per digit class (0-9) |

### Neuron Model

```
                    SINGLE NEURON
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                         â”‚
    â”‚   inputs (from previous layer)          â”‚
    â”‚      â†“                                  â”‚
    â”‚   xâ‚ â”€â”€wâ‚â”€â”€â”                           â”‚
    â”‚   xâ‚‚ â”€â”€wâ‚‚â”€â”€â”¼â”€â”€â–º Î£ â”€â”€(+b)â”€â”€â–º Ïƒ â”€â”€â–º output
    â”‚   xâ‚ƒ â”€â”€wâ‚ƒâ”€â”€â”˜    â”‚                â”‚     â”‚
    â”‚   ...           â”‚                â”‚     â”‚
    â”‚                 â”‚                â”‚     â”‚
    â”‚            weighted          sigmoid   â”‚
    â”‚              sum            activation â”‚
    â”‚                                         â”‚
    â”‚   z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + wâ‚ƒxâ‚ƒ + ... + b    â”‚
    â”‚   a = Ïƒ(z)                              â”‚
    â”‚                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Weights and Biases

### What Are Weights?

Weights determine **how much each input influences the output**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WEIGHT INTERPRETATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Large positive weight (+2.5):  Input strongly ACTIVATES the neuron    â”‚
â”‚  Large negative weight (-2.5):  Input strongly INHIBITS the neuron     â”‚
â”‚  Small weight (~0):             Input has little effect                â”‚
â”‚                                                                         â”‚
â”‚  Example: A hidden neuron that detects "horizontal lines"              â”‚
â”‚                                                                         â”‚
â”‚      Input Image        Weights (28Ã—28)        Result                  â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚        â”‚+++++++++â”‚           High                     â”‚
â”‚      â”‚         â”‚   Ã—    â”‚         â”‚     =     activation               â”‚
â”‚      â”‚         â”‚        â”‚         â”‚           (line detected!)         â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Are Biases?

Biases control **how easily a neuron activates**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BIAS INTERPRETATION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  z = Î£(wáµ¢xáµ¢) + b                                                       â”‚
â”‚                                                                         â”‚
â”‚  Positive bias (+5):  Neuron activates easily (even with weak input)   â”‚
â”‚  Negative bias (-5):  Neuron is "skeptical" (needs strong input)       â”‚
â”‚  Zero bias (0):       Neutral threshold                                â”‚
â”‚                                                                         â”‚
â”‚              Sigmoid Output                                             â”‚
â”‚         1 â”¤         â•­â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚           â”‚        â•±                                                    â”‚
â”‚       0.5 â”¤â”€â”€â”€â”€â”€â”€â”€â•³â”€â”€â”€â”€â”€â”€â”€â”€   â† b shifts this curve left/right        â”‚
â”‚           â”‚      â•±                                                      â”‚
â”‚         0 â”¤â”€â”€â”€â”€â”€â•¯                                                       â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚              -10   0   +10   z                                         â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Parameter Count

```
Network [784, 30, 10]:

Layer 1 (input â†’ hidden):
  Weights WÂ¹: 30 Ã— 784 = 23,520 parameters
  Biases bÂ¹:  30 Ã— 1   = 30 parameters
  
Layer 2 (hidden â†’ output):
  Weights WÂ²: 10 Ã— 30  = 300 parameters
  Biases bÂ²:  10 Ã— 1   = 10 parameters

TOTAL: 23,520 + 30 + 300 + 10 = 23,860 learnable parameters
```

### Weight Matrix Dimensions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEIGHT MATRIX SHAPES                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  W[layer] has shape: (neurons_out, neurons_in)                         â”‚
â”‚                                                                         â”‚
â”‚  WÂ¹ shape: (30, 784)                                                   â”‚
â”‚                                                                         â”‚
â”‚       â”Œâ”€ 784 columns (one per input pixel) â”€â”                          â”‚
â”‚       â†“                                     â†“                          â”‚
â”‚  30  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                         â”‚
â”‚  rowsâ•‘  wâ‚€,â‚€   wâ‚€,â‚   wâ‚€,â‚‚  ...  wâ‚€,â‚‡â‚ˆâ‚ƒ    â•‘ â† weights to neuron 0   â”‚
â”‚  (oneâ•‘  wâ‚,â‚€   wâ‚,â‚   wâ‚,â‚‚  ...  wâ‚,â‚‡â‚ˆâ‚ƒ    â•‘ â† weights to neuron 1   â”‚
â”‚  per â•‘  ...    ...    ...   ...  ...        â•‘                         â”‚
â”‚  out)â•‘  wâ‚‚â‚‰,â‚€  wâ‚‚â‚‰,â‚  wâ‚‚â‚‰,â‚‚ ... wâ‚‚â‚‰,â‚‡â‚ˆâ‚ƒ   â•‘ â† weights to neuron 29  â”‚
â”‚      â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                         â”‚
â”‚                                                                         â”‚
â”‚  Each ROW contains all weights connecting to ONE hidden neuron         â”‚
â”‚  (This is why we can visualize each row as a 28Ã—28 "filter")          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Sigmoid Activation

### The Sigmoid Function

```
                    1
Ïƒ(z) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         1 + e^(-z)
```

### Properties

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SIGMOID PROPERTIES                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  1. Output range: (0, 1) - interpretable as probability                â”‚
â”‚                                                                         â”‚
â”‚  2. Smooth & differentiable - needed for gradient descent              â”‚
â”‚                                                                         â”‚
â”‚  3. Key values:                                                         â”‚
â”‚     Ïƒ(-âˆ) â†’ 0     (very negative input â†’ output near 0)                â”‚
â”‚     Ïƒ(0)  = 0.5   (zero input â†’ output exactly 0.5)                    â”‚
â”‚     Ïƒ(+âˆ) â†’ 1     (very positive input â†’ output near 1)                â”‚
â”‚                                                                         â”‚
â”‚  4. Derivative: Ïƒ'(z) = Ïƒ(z) Ã— (1 - Ïƒ(z))                              â”‚
â”‚     Maximum at z=0: Ïƒ'(0) = 0.25                                       â”‚
â”‚     Vanishes for large |z| (vanishing gradient problem!)               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visualization

```
    Sigmoid Ïƒ(z)                    Derivative Ïƒ'(z)
    
  1 â”¤         â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€          0.25â”¤      â•­â•®
    â”‚        â•±                        â”‚     â•±  â•²
    â”‚       â•±                         â”‚    â•±    â•²
0.5 â”¤â”€â”€â”€â”€â”€â”€â•³                          â”‚   â•±      â•²
    â”‚     â•±                           â”‚  â•±        â•²
    â”‚    â•±                            â”‚ â•±          â•²
  0 â”¤â”€â”€â”€â•¯                           0 â”¼â•±            â•²â”€â”€â”€
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      -5    0    +5   z                 -5    0    +5   z
```

---

## 5. Feedforward Computation

### The Algorithm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEEDFORWARD STEP BY STEP                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Start with input aâ° = x (the image, 784Ã—1)                            â”‚
â”‚                                                                         â”‚
â”‚  For each layer l = 1, 2, ..., L:                                      â”‚
â”‚                                                                         â”‚
â”‚      zË¡ = WË¡ Â· aË¡â»Â¹ + bË¡     â† weighted sum                           â”‚
â”‚      aË¡ = Ïƒ(zË¡)               â† apply activation                       â”‚
â”‚                                                                         â”‚
â”‚  Output: aá´¸ (the prediction, 10Ã—1)                                     â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Concrete Example

```
Network [784, 30, 10] with an image of "5":

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  INPUT LAYER                                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                            â”‚
â”‚  aâ° = x = [0.0, 0.0, 0.3, 0.9, ..., 0.0]áµ€   (784Ã—1 image vector)      â”‚
â”‚                                                                         â”‚
â”‚                     â†“                                                   â”‚
â”‚            zÂ¹ = WÂ¹ Â· aâ° + bÂ¹                                           â”‚
â”‚            (30Ã—784)Â·(784Ã—1) + (30Ã—1) = (30Ã—1)                          â”‚
â”‚                     â†“                                                   â”‚
â”‚            aÂ¹ = Ïƒ(zÂ¹)                        (30Ã—1 hidden activations) â”‚
â”‚                                                                         â”‚
â”‚  HIDDEN LAYER                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  aÂ¹ = [0.2, 0.8, 0.1, 0.9, ..., 0.3]áµ€       (30Ã—1)                    â”‚
â”‚                                                                         â”‚
â”‚                     â†“                                                   â”‚
â”‚            zÂ² = WÂ² Â· aÂ¹ + bÂ²                                           â”‚
â”‚            (10Ã—30)Â·(30Ã—1) + (10Ã—1) = (10Ã—1)                            â”‚
â”‚                     â†“                                                   â”‚
â”‚            aÂ² = Ïƒ(zÂ²)                        (10Ã—1 output activations) â”‚
â”‚                                                                         â”‚
â”‚  OUTPUT LAYER                                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚  aÂ² = [0.01, 0.02, 0.05, 0.08, 0.03, 0.75, 0.02, 0.02, 0.01, 0.01]áµ€   â”‚
â”‚         â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘     â†‘       â”‚
â”‚        P(0)  P(1)  P(2)  P(3)  P(4)  P(5)  P(6)  P(7)  P(8)  P(9)     â”‚
â”‚                                       â”‚                                 â”‚
â”‚                              Highest! â† Prediction = 5                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Cost Function

### Quadratic Cost (Mean Squared Error)

```
         1
C = â”€â”€â”€â”€â”€â”€â”€ Î£ â€–y - aâ€–Â²
      2n    x

Where:
  n = number of training examples
  y = target output (one-hot vector)
  a = actual output (network prediction)
  â€–Â·â€– = Euclidean norm
```

### Example Calculation

```
Target y = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]áµ€   (digit 5)
Output a = [0.1, 0.1, 0.1, 0.1, 0.1, 0.4, 0.05, 0.05, 0.05, 0.05]áµ€

Error vector (y - a):
  [0-0.1, 0-0.1, 0-0.1, 0-0.1, 0-0.1, 1-0.4, 0-0.05, ...]
= [-0.1, -0.1, -0.1, -0.1, -0.1, 0.6, -0.05, -0.05, -0.05, -0.05]

â€–y - aâ€–Â² = 0.01 + 0.01 + 0.01 + 0.01 + 0.01 + 0.36 + 0.0025Ã—4
         = 0.05 + 0.36 + 0.01
         = 0.42

Cost for this example: C = 0.42 / 2 = 0.21
```

---

## 7. Backpropagation

### The Four Fundamental Equations

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    THE BACKPROPAGATION EQUATIONS                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                         â•‘
â•‘  (BP1)  Î´á´¸ = âˆ‡â‚C âŠ™ Ïƒ'(zá´¸)                                              â•‘
â•‘         â”‚                                                               â•‘
â•‘         â””â”€â–º Error at OUTPUT layer                                       â•‘
â•‘             (how much output affects cost Ã— how much z affects output)  â•‘
â•‘                                                                         â•‘
â•‘  (BP2)  Î´Ë¡ = (WË¡âºÂ¹)áµ€ Â· Î´Ë¡âºÂ¹ âŠ™ Ïƒ'(zË¡)                                   â•‘
â•‘         â”‚                                                               â•‘
â•‘         â””â”€â–º Error at layer l in terms of layer l+1                      â•‘
â•‘             (propagate error BACKWARD through weights)                  â•‘
â•‘                                                                         â•‘
â•‘  (BP3)  âˆ‚C/âˆ‚bË¡â±¼ = Î´Ë¡â±¼                                                   â•‘
â•‘         â”‚                                                               â•‘
â•‘         â””â”€â–º Gradient for biases = the error itself!                     â•‘
â•‘                                                                         â•‘
â•‘  (BP4)  âˆ‚C/âˆ‚wË¡â±¼â‚– = aË¡â»Â¹â‚– Â· Î´Ë¡â±¼                                          â•‘
â•‘         â”‚                                                               â•‘
â•‘         â””â”€â–º Gradient for weights = input activation Ã— error             â•‘
â•‘                                                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Key: âŠ™ = element-wise (Hadamard) product
```

### Visual Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKPROPAGATION FLOW                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚           FORWARD PASS (compute activations)                            â”‚
â”‚           â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–º                           â”‚
â”‚                                                                         â”‚
â”‚      aâ° â”€â”€â”€â”€WÂ¹â”€â”€â”€â”€â–º aÂ¹ â”€â”€â”€â”€WÂ²â”€â”€â”€â”€â–º aÂ² â”€â”€â”€â”€â”€â”€â”€â”€â–º Cost C                 â”‚
â”‚     input         hidden         output                                 â”‚
â”‚                                                                         â”‚
â”‚                                                                         â”‚
â”‚           â—„â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                           â”‚
â”‚           BACKWARD PASS (compute gradients)                             â”‚
â”‚                                                                         â”‚
â”‚                                                                         â”‚
â”‚  Step 1: Compute output error                                           â”‚
â”‚          Î´Â² = (aÂ² - y) âŠ™ Ïƒ'(zÂ²)                                        â”‚
â”‚                     â†‘                                                   â”‚
â”‚                     â”‚                                                   â”‚
â”‚  Step 2: Propagate backward                                             â”‚
â”‚          Î´Â¹ = (WÂ²)áµ€ Â· Î´Â² âŠ™ Ïƒ'(zÂ¹)                                      â”‚
â”‚                     â†‘                                                   â”‚
â”‚                     â”‚                                                   â”‚
â”‚  Step 3: Compute gradients                                              â”‚
â”‚          âˆ‚C/âˆ‚WÂ² = Î´Â² Â· (aÂ¹)áµ€                                           â”‚
â”‚          âˆ‚C/âˆ‚bÂ² = Î´Â²                                                    â”‚
â”‚          âˆ‚C/âˆ‚WÂ¹ = Î´Â¹ Â· (aâ°)áµ€                                           â”‚
â”‚          âˆ‚C/âˆ‚bÂ¹ = Î´Â¹                                                    â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why It Works: Chain Rule

```
Cost depends on output:           C = f(aÂ²)
Output depends on z:              aÂ² = Ïƒ(zÂ²)
z depends on weights:             zÂ² = WÂ² Â· aÂ¹ + bÂ²

By chain rule:
âˆ‚C/âˆ‚WÂ² = âˆ‚C/âˆ‚aÂ² Â· âˆ‚aÂ²/âˆ‚zÂ² Â· âˆ‚zÂ²/âˆ‚WÂ²
         \_____/   \_____/   \_____/
            â”‚         â”‚         â”‚
            â”‚         â”‚         â””â”€â”€ = aÂ¹áµ€ (input to this layer)
            â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = Ïƒ'(zÂ²) (sigmoid derivative)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = (aÂ² - y) (cost derivative)

Combined: âˆ‚C/âˆ‚WÂ² = (aÂ² - y) âŠ™ Ïƒ'(zÂ²) Â· (aÂ¹)áµ€ = Î´Â² Â· (aÂ¹)áµ€
```

---

## 8. Stochastic Gradient Descent

### The Update Rule

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GRADIENT DESCENT UPDATE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  w_new = w_old - Î· Ã— âˆ‚C/âˆ‚w                                             â”‚
â”‚  b_new = b_old - Î· Ã— âˆ‚C/âˆ‚b                                             â”‚
â”‚                                                                         â”‚
â”‚  Where Î· (eta) is the LEARNING RATE                                     â”‚
â”‚                                                                         â”‚
â”‚  Intuition: Move in the opposite direction of the gradient             â”‚
â”‚             (gradient points "uphill", we want to go "downhill")       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mini-Batch SGD

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MINI-BATCH APPROACH                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Instead of computing gradient over ALL 50,000 training examples:      â”‚
â”‚                                                                         â”‚
â”‚  1. Shuffle the training data                                           â”‚
â”‚                                                                         â”‚
â”‚  2. Split into mini-batches (e.g., 10 examples each)                   â”‚
â”‚     [batch_1] [batch_2] [batch_3] ... [batch_5000]                     â”‚
â”‚                                                                         â”‚
â”‚  3. For each batch:                                                     â”‚
â”‚     - Compute gradient for each example in batch                        â”‚
â”‚     - Average the gradients                                             â”‚
â”‚     - Update weights once                                               â”‚
â”‚                                                                         â”‚
â”‚  Benefits:                                                              â”‚
â”‚  âœ“ Faster: Don't wait for all 50,000 examples                          â”‚
â”‚  âœ“ Regularization: Noise helps escape local minima                     â”‚
â”‚  âœ“ Memory: Only need to hold batch_size examples                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Learning Rate Effect

```
         Cost                              Cost                    Cost
           â”‚                                â”‚                        â”‚
           â”‚  Î· too HIGH                    â”‚  Î· just RIGHT          â”‚  Î· too LOW
           â”‚  (unstable)                    â”‚  (converges)           â”‚  (slow)
           â”‚                                â”‚                        â”‚
           â”‚    â•±â•²  â•±â•²                     â”‚ â•²                      â”‚ â•²
           â”‚   â•±  â•²â•±  â•²   oscillates!      â”‚  â•²                     â”‚  â•²
           â”‚  â•±        â•²                   â”‚   â•²                    â”‚   â•²
           â”‚ â•±                             â”‚    â•²__                 â”‚    â•²
           â”‚â•±                              â”‚       â•²___             â”‚     â•²___
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                 epochs                          epochs                   epochs
```

---

## 9. Training Process

### Complete Training Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING ALGORITHM                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  FOR epoch = 1 to num_epochs:                                          â”‚
â”‚  â”‚                                                                      â”‚
â”‚  â”‚   1. Shuffle training data                                          â”‚
â”‚  â”‚                                                                      â”‚
â”‚  â”‚   2. Split into mini-batches                                        â”‚
â”‚  â”‚                                                                      â”‚
â”‚  â”‚   3. FOR each mini_batch:                                           â”‚
â”‚  â”‚      â”‚                                                               â”‚
â”‚  â”‚      â”‚   Initialize: nabla_w = 0, nabla_b = 0                       â”‚
â”‚  â”‚      â”‚                                                               â”‚
â”‚  â”‚      â”‚   FOR each (x, y) in mini_batch:                             â”‚
â”‚  â”‚      â”‚   â”‚                                                          â”‚
â”‚  â”‚      â”‚   â”‚   â€¢ Feedforward: compute all a and z                     â”‚
â”‚  â”‚      â”‚   â”‚   â€¢ Backprop: compute Î´ for each layer                   â”‚
â”‚  â”‚      â”‚   â”‚   â€¢ Accumulate: nabla_w += âˆ‚C/âˆ‚w, nabla_b += âˆ‚C/âˆ‚b      â”‚
â”‚  â”‚      â”‚   â”‚                                                          â”‚
â”‚  â”‚      â”‚   END FOR                                                    â”‚
â”‚  â”‚      â”‚                                                               â”‚
â”‚  â”‚      â”‚   Update weights:                                            â”‚
â”‚  â”‚      â”‚   w = w - (Î·/batch_size) Ã— nabla_w                           â”‚
â”‚  â”‚      â”‚   b = b - (Î·/batch_size) Ã— nabla_b                           â”‚
â”‚  â”‚      â”‚                                                               â”‚
â”‚  â”‚      END FOR                                                        â”‚
â”‚  â”‚                                                                      â”‚
â”‚  â”‚   4. Optionally: evaluate on test set, print progress               â”‚
â”‚  â”‚                                                                      â”‚
â”‚  END FOR                                                               â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Typical Training Results

```
Network [784, 30, 10], Î·=3.0, batch_size=10:

Epoch  0: 9098 / 10000 = 90.98%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â”‚
Epoch  1: 9199 / 10000 = 91.99%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  2: 9319 / 10000 = 93.19%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  3: 9332 / 10000 = 93.32%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  4: 9382 / 10000 = 93.82%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  5: 9345 / 10000 = 93.45%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  6: 9415 / 10000 = 94.15%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  7: 9392 / 10000 = 93.92%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â”‚
Epoch  8: 9423 / 10000 = 94.23%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â”‚
Epoch  9: 9431 / 10000 = 94.31%  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â”‚

Final accuracy: ~94%
```

---

## 10. Code Reference

### Creating a Network

```python
import network

# Create network with 784 input, 30 hidden, 10 output neurons
net = network.Network([784, 30, 10])

# Access weights and biases
print(net.weights[0].shape)  # (30, 784) - input to hidden
print(net.weights[1].shape)  # (10, 30)  - hidden to output
print(net.biases[0].shape)   # (30, 1)   - hidden layer
print(net.biases[1].shape)   # (10, 1)   - output layer
```

### Feedforward

```python
def feedforward(self, a):
    """Return the output of the network if 'a' is input."""
    for b, w in zip(self.biases, self.weights):
        a = sigmoid(np.dot(w, a) + b)
    return a
```

### Training

```python
import mnist_loader

# Load data
training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
training_data = list(training_data)
test_data = list(test_data)

# Create and train network
net = network.Network([784, 30, 10])
net.SGD(training_data, epochs=10, mini_batch_size=10, eta=3.0, test_data=test_data)
```

### Making Predictions

```python
# Get a test image
x, y = test_data[0]

# Feedforward to get output
output = net.feedforward(x)

# Prediction is the index of highest activation
prediction = np.argmax(output)
print(f"Predicted: {prediction}, Actual: {y}")
```

---

## 11. Exercises

### Exercise 1: Experiment with Hidden Layer Size

Try different hidden layer sizes and observe the effect:

```python
for hidden_size in [10, 30, 100, 300]:
    net = network.Network([784, hidden_size, 10])
    net.SGD(training_data, epochs=5, mini_batch_size=10, eta=3.0, test_data=test_data)
```

**Questions:**
- How does accuracy change with hidden layer size?
- How does training time change?
- Is bigger always better?

### Exercise 2: Learning Rate Exploration

Try different learning rates:

```python
for eta in [0.1, 1.0, 3.0, 10.0, 30.0]:
    net = network.Network([784, 30, 10])
    net.SGD(training_data, epochs=5, mini_batch_size=10, eta=eta, test_data=test_data)
```

**Questions:**
- What happens with very small Î·?
- What happens with very large Î·?
- What's the "sweet spot"?

### Exercise 3: Manual Feedforward

Implement feedforward without using the class method:

```python
def my_feedforward(weights, biases, x):
    """Compute network output for input x."""
    a = x
    for w, b in zip(weights, biases):
        z = np.dot(w, a) + b
        a = 1.0 / (1.0 + np.exp(-z))  # sigmoid
    return a

# Test it
x, y = training_data[0]
my_output = my_feedforward(net.weights, net.biases, x)
net_output = net.feedforward(x)
print(f"Match: {np.allclose(my_output, net_output)}")
```

### Exercise 4: Visualize Training Progress

Modify SGD to track accuracy over epochs and plot it:

```python
import matplotlib.pyplot as plt

accuracies = []
for epoch in range(30):
    net.SGD(training_data, epochs=1, mini_batch_size=10, eta=3.0)
    accuracy = net.evaluate(test_data) / len(test_data)
    accuracies.append(accuracy)

plt.plot(accuracies)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Progress')
plt.show()
```

---

## Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEVEL 2 KEY CONCEPTS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ARCHITECTURE                                                           â”‚
â”‚  â€¢ Layers: input â†’ hidden â†’ output                                     â”‚
â”‚  â€¢ Weights connect layers, biases shift activations                    â”‚
â”‚  â€¢ [784, 30, 10] has 23,860 parameters                                 â”‚
â”‚                                                                         â”‚
â”‚  FEEDFORWARD                                                            â”‚
â”‚  â€¢ z = WÂ·a + b (weighted sum)                                          â”‚
â”‚  â€¢ a = Ïƒ(z) (apply sigmoid)                                            â”‚
â”‚  â€¢ Repeat for each layer                                               â”‚
â”‚                                                                         â”‚
â”‚  BACKPROPAGATION                                                        â”‚
â”‚  â€¢ Compute error Î´ at output layer                                     â”‚
â”‚  â€¢ Propagate error backward through layers                             â”‚
â”‚  â€¢ Compute gradients âˆ‚C/âˆ‚w and âˆ‚C/âˆ‚b                                   â”‚
â”‚                                                                         â”‚
â”‚  TRAINING (SGD)                                                         â”‚
â”‚  â€¢ Split data into mini-batches                                        â”‚
â”‚  â€¢ Update: w = w - Î· Ã— gradient                                        â”‚
â”‚  â€¢ Repeat for many epochs                                              â”‚
â”‚                                                                         â”‚
â”‚  RESULT: ~94% accuracy on MNIST with simple network!                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Exploration Scripts

| Script | Description |
|--------|-------------|
| `src/level2/1_neural_network.py` | Step-by-step network implementation |
| `src/level2/2_interactive_digit_recognition.py` | Draw digits and see real-time predictions |

### Interactive Demo Features

The `2_interactive_digit_recognition.py` demo includes several improvements:

**Model Caching**
- Saves trained model to `digit_model.pkl`
- Loads instantly on subsequent runs (no retraining)

**MNIST-Style Preprocessing**
```python
def preprocess_image(self):
    """
    Centers the drawn digit like MNIST:
    1. Find bounding box of non-zero pixels
    2. Crop to just the digit
    3. Resize to fit in 20Ã—20 box (preserving aspect ratio)
    4. Center in 28Ã—28 image
    5. Apply Gaussian blur for anti-aliasing
    """
```

**28Ã—28 Preview**
- Shows what the network actually sees after preprocessing
- Helps understand why some digits are misclassified

**Larger Network**
- Architecture: `[784, 100, 10]` (vs original `[784, 30, 10]`)
- 15 epochs of training
- Achieves ~96.5% accuracy

### Running the Demo

```bash
cd /home/alcidesticlla/Documents/MOOC/mniels/neural-networks-and-deep-learning/src
python level2/2_interactive_digit_recognition.py
```

---

## Next Steps: Level 3

Level 3 will improve our network with:

1. **Cross-Entropy Cost** - Faster learning, no vanishing gradients
2. **L2 Regularization** - Prevent overfitting
3. **Better Initialization** - Xavier/He initialization
4. **Learning Rate Schedules** - Adaptive learning rates

These techniques can push accuracy from 96% to 98%+!

