# Level 4: Convolutional Neural Networks (CNNs)

> **Goal**: Understand how CNNs exploit spatial structure in images to achieve state-of-the-art accuracy (~99%+)

---

## ğŸ“š Overview

CNNs are the breakthrough architecture that revolutionized computer vision. They achieve better results than fully-connected networks by:

1. **Understanding spatial relationships** in images
2. **Sharing weights** across the image (fewer parameters)
3. **Building translation invariance** (detect features anywhere)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CNN vs Fully Connected                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚  Fully Connected (Level 3):                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ 784 inputs    â”‚â”€â”€â”€â”€â”€â”€â”‚ 100 hidden  â”‚â”€â”€â”€â”€â”€â”€â”‚ 10 outputâ”‚                â”‚
â”‚  â”‚ (flattened)   â”‚      â”‚ (dense)     â”‚      â”‚          â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚  Parameters: 784Ã—100 + 100Ã—10 = 79,400                                   â”‚
â”‚  Accuracy: ~97.7%                                                         â”‚
â”‚                                                                           â”‚
â”‚  CNN (Level 4):                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 28Ã—28     â”‚â”€â”€â”€â”‚Conv 5Ã—5  â”‚â”€â”€â”€â”‚Pool 2Ã—2  â”‚â”€â”€â”€â”‚ FC 100 â”‚â”€â”€â”€â”‚ 10 out â”‚  â”‚
â”‚  â”‚ image     â”‚   â”‚20 filtersâ”‚   â”‚12Ã—12Ã—20  â”‚   â”‚        â”‚   â”‚        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Parameters: 20Ã—(5Ã—5) + 20Ã—12Ã—12Ã—100 + ... â‰ˆ 30,000                      â”‚
â”‚  Accuracy: ~99%+ â­                                                       â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Key Concepts

### 1. Local Receptive Fields

Instead of connecting every pixel to every neuron, CNNs use **small windows** (filters) that scan across the image:

```
Input Image (28Ã—28)                    First Hidden Neuron
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”
â”‚ â”Œâ”€â”€â”€â”€â”€â”             â”‚                     â”‚   â”‚
â”‚ â”‚ 5Ã—5 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ â—‹ â”‚
â”‚ â”‚fieldâ”‚             â”‚    25 weights       â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜             â”‚    + 1 bias         â””â”€â”€â”€â”˜
â”‚                     â”‚                   
â”‚                     â”‚                    Second Hidden Neuron
â”‚   â”Œâ”€â”€â”€â”€â”€â”           â”‚                     â”Œâ”€â”€â”€â”
â”‚   â”‚ 5Ã—5 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚ â—‹ â”‚
â”‚   â”‚     â”‚           â”‚                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”˜           â”‚                     â””â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    
    â†‘                                      (same weights!)
    Stride = 1 pixel
```

**Key insight**: The 5Ã—5 window slides across the entire image, creating a **feature map** (24Ã—24 for 28Ã—28 input with 5Ã—5 filter).

### 2. Weight Sharing (Shared Filters)

The SAME weights are used for every position in the image:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         WEIGHT SHARING                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   Filter (5Ã—5 = 25 weights + 1 bias):                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚   â”‚ wâ‚  wâ‚‚  wâ‚ƒ  wâ‚„  wâ‚…  â”‚                                              â”‚
â”‚   â”‚ wâ‚†  wâ‚‡  wâ‚ˆ  wâ‚‰  wâ‚â‚€ â”‚     These 26 values are used                 â”‚
â”‚   â”‚ wâ‚â‚ wâ‚â‚‚ wâ‚â‚ƒ wâ‚â‚„ wâ‚â‚…â”‚     for ALL positions!                       â”‚
â”‚   â”‚ wâ‚â‚† wâ‚â‚‡ wâ‚â‚ˆ wâ‚â‚‰ wâ‚‚â‚€â”‚                                              â”‚
â”‚   â”‚ wâ‚‚â‚ wâ‚‚â‚‚ wâ‚‚â‚ƒ wâ‚‚â‚„ wâ‚‚â‚…â”‚                                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚                                                                          â”‚
â”‚   Without sharing: 24Ã—24 positions Ã— 26 = 14,976 parameters             â”‚
â”‚   With sharing:    26 parameters only!   â† 576Ã— fewer!                  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why it works**: A feature (like an edge or curve) should be detectable anywhere in the image.

### 3. Feature Maps

One filter learns ONE feature. We use MULTIPLE filters to learn different features:

```
Input (28Ã—28)     20 Filters (5Ã—5 each)        20 Feature Maps (24Ã—24 each)
                                              
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”
â”‚           â”‚     â”‚ Filter 1: â”€      â”‚       â”‚ Map 1     â”‚ Map 2     â”‚...â”‚
â”‚   "7"     â”‚ â”€â”€â†’ â”‚ Filter 2: |      â”‚  â”€â”€â†’  â”‚ (edges)   â”‚ (curves)  â”‚   â”‚
â”‚           â”‚     â”‚ Filter 3: /      â”‚       â”‚           â”‚           â”‚   â”‚
â”‚           â”‚     â”‚ ...              â”‚       â”‚           â”‚           â”‚   â”‚
â”‚           â”‚     â”‚ Filter 20: â—‹     â”‚       â”‚           â”‚           â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜

                  Each filter: 26 params      Output: 24Ã—24Ã—20 = 11,520 values
                  Total: 20Ã—26 = 520 params
```

### 4. Pooling Layers (Max Pooling)

After convolution, we **downsample** using max pooling:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MAX POOLING (2Ã—2)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   Feature Map (24Ã—24)              Pooled Map (12Ã—12)                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚   â”‚ 1  3 â”‚ 2  4 â”‚...â”‚              â”‚                 â”‚                   â”‚
â”‚   â”‚ 0  2 â”‚ 1  3 â”‚   â”‚     max      â”‚  3    4   ...   â”‚                   â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤   â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚                 â”‚                   â”‚
â”‚   â”‚ 5  6 â”‚ 7  8 â”‚   â”‚    2Ã—2       â”‚  6    8   ...   â”‚                   â”‚
â”‚   â”‚ 4  1 â”‚ 2  5 â”‚   â”‚              â”‚                 â”‚                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â†‘                                   â†‘                             â”‚
â”‚   Take max from                       Half the size!                      â”‚
â”‚   each 2Ã—2 region                    (24/2 = 12)                         â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- Reduces computation (smaller feature maps)
- Adds **translation invariance** (small shifts don't change output)
- Controls overfitting

### 5. ReLU Activation

CNNs typically use **ReLU** instead of sigmoid:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ACTIVATION FUNCTIONS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   Sigmoid: Ïƒ(z) = 1/(1+e^(-z))         ReLU: f(z) = max(0, z)            â”‚
â”‚                                                                           â”‚
â”‚       1.0 â”¤      â•­â”€â”€â”€â”€â”€â”€â”€â”€             âˆ  â”¤           /                  â”‚
â”‚           â”‚     /                         â”‚          /                   â”‚
â”‚       0.5 â”¤    â”‚                          â”‚         /                    â”‚
â”‚           â”‚   /                           â”‚        /                     â”‚
â”‚       0.0 â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             0  â”¼â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚           -4    0    4                      -4    0    4                  â”‚
â”‚                                                                           â”‚
â”‚   Problem: Vanishing gradient         âœ“ No vanishing gradient            â”‚
â”‚            when |z| is large          âœ“ Computationally cheap            â”‚
â”‚                                       âœ“ Sparse activations               â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6. Dropout Regularization

During training, randomly "drop" neurons:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              DROPOUT                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   Training (p=0.5):                    Testing:                           â”‚
â”‚                                                                           â”‚
â”‚   â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                       â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                      â”‚
â”‚   â—‹ â”€â”€â”€â”€â•³     â—  (dropped)            â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹  (Ã—0.5)              â”‚
â”‚   â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                       â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                      â”‚
â”‚   â—‹ â”€â”€â”€â”€â•³     â—  (dropped)            â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹  (Ã—0.5)              â”‚
â”‚   â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                       â—‹ â”€â”€â”€â”€â”€â”€â”€â”€â†’ â—‹                      â”‚
â”‚                                                                           â”‚
â”‚   Each training step uses              At test time, use all              â”‚
â”‚   a different random subset           neurons but scale outputs          â”‚
â”‚                                                                           â”‚
â”‚   Effect: Prevents co-adaptation, like training ensemble of networks     â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7. Softmax Output Layer

For classification, use softmax to get probabilities:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              SOFTMAX                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   softmax(zâ±¼) = e^(zâ±¼) / Î£â‚– e^(zâ‚–)                                       â”‚
â”‚                                                                           â”‚
â”‚   Raw outputs (logits):        After softmax:                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚   â”‚ zâ‚€ =  2.1         â”‚       â”‚ P(0) = 0.01        â”‚                    â”‚
â”‚   â”‚ zâ‚ = -0.5         â”‚       â”‚ P(1) = 0.00        â”‚                    â”‚
â”‚   â”‚ zâ‚‚ =  0.3         â”‚       â”‚ P(2) = 0.02        â”‚                    â”‚
â”‚   â”‚ zâ‚ƒ =  1.2         â”‚       â”‚ P(3) = 0.05        â”‚                    â”‚
â”‚   â”‚ zâ‚„ = -1.0         â”‚       â”‚ P(4) = 0.01        â”‚                    â”‚
â”‚   â”‚ zâ‚… =  0.8         â”‚  â”€â”€â†’  â”‚ P(5) = 0.03        â”‚                    â”‚
â”‚   â”‚ zâ‚† =  3.9         â”‚       â”‚ P(6) = 0.73  â† max â”‚                    â”‚
â”‚   â”‚ zâ‚‡ =  2.5         â”‚       â”‚ P(7) = 0.18        â”‚                    â”‚
â”‚   â”‚ zâ‚ˆ = -0.2         â”‚       â”‚ P(8) = 0.01        â”‚                    â”‚
â”‚   â”‚ zâ‚‰ =  0.1         â”‚       â”‚ P(9) = 0.02        â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                     Sum = 1.00                            â”‚
â”‚                                                                           â”‚
â”‚   Used with Cross-Entropy Loss: L = -log(P(correct class))               â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Complete CNN Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         COMPLETE CNN ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  INPUT              CONV1 + POOL1       CONV2 + POOL2       FC + OUTPUT      â”‚
â”‚  (1Ã—28Ã—28)          (20Ã—12Ã—12)          (40Ã—4Ã—4)            (100) â†’ (10)     â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â–‘â–‘â–‘â–‘â–‘  â”‚       â”‚ â–‘ â–‘ â–‘ â–‘     â”‚    â”‚ â–‘â–‘â–‘â–‘         â”‚    â”‚            â”‚   â”‚
â”‚  â”‚  â–‘â–ˆâ–ˆâ–ˆâ–‘  â”‚ â”€â”€â”€â†’  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘      â”‚ â”€â†’ â”‚ â–‘â–‘â–‘â–‘         â”‚ â”€â†’ â”‚  FC: 100   â”‚ â”€â†’â”‚
â”‚  â”‚  â–‘â–‘â–ˆâ–‘â–‘  â”‚ Conv  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘      â”‚Convâ”‚ â–‘â–‘â–‘â–‘         â”‚    â”‚  neurons   â”‚   â”‚
â”‚  â”‚  â–‘â–‘â–ˆâ–‘â–‘  â”‚ 5Ã—5   â”‚ ...         â”‚5Ã—5 â”‚              â”‚    â”‚            â”‚   â”‚
â”‚  â”‚  â–‘â–‘â–ˆâ–‘â–‘  â”‚       â”‚             â”‚    â”‚              â”‚    â”‚  Dropout   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚                  â”‚                    â”‚                   â”‚          â”‚
â”‚       â”‚            20 filters           40 filters          Softmax          â”‚
â”‚       â”‚            + ReLU               + ReLU              Output          â”‚
â”‚       â”‚            + MaxPool 2Ã—2        + MaxPool 2Ã—2       (10 classes)    â”‚
â”‚       â”‚                                                                      â”‚
â”‚       â”‚                                                                      â”‚
â”‚  784 pixels        20Ã—(5Ã—5Ã—1) = 500     40Ã—(5Ã—5Ã—20) = 20,000                â”‚
â”‚                    â†’ 20Ã—12Ã—12 = 2,880   â†’ 40Ã—4Ã—4 = 640                      â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dimension calculations:
- Input: 28Ã—28 â†’ Conv 5Ã—5 â†’ 24Ã—24 â†’ Pool 2Ã—2 â†’ 12Ã—12
- Conv2: 12Ã—12 â†’ Conv 5Ã—5 â†’ 8Ã—8 â†’ Pool 2Ã—2 â†’ 4Ã—4
- Flatten: 40Ã—4Ã—4 = 640 â†’ FC â†’ 100 â†’ 10
```

---

## ğŸ“ˆ Performance Progression

| Architecture | Accuracy | Parameters |
|-------------|----------|------------|
| FC [784â†’100â†’10] (Level 3) | ~97.7% | ~80K |
| Conv(20) â†’ FC(100) â†’ 10 | ~98.5% | ~30K |
| Conv(20) â†’ Conv(40) â†’ FC(100) â†’ 10 | ~99.0% | ~35K |
| + ReLU activation | ~99.2% | ~35K |
| + Dropout + 1000 FC neurons | ~99.6% | ~800K |

---

## ğŸ PyTorch Implementation

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)   # 28â†’24
        self.pool = nn.MaxPool2d(2, 2)                 # 24â†’12
        self.conv2 = nn.Conv2d(20, 40, kernel_size=5)  # 12â†’8â†’4
        
        # Fully connected layers
        self.fc1 = nn.Linear(40 * 4 * 4, 100)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(100, 10)
    
    def forward(self, x):
        # Conv block 1: Conv â†’ ReLU â†’ Pool
        x = self.pool(F.relu(self.conv1(x)))  # 1Ã—28Ã—28 â†’ 20Ã—12Ã—12
        
        # Conv block 2: Conv â†’ ReLU â†’ Pool
        x = self.pool(F.relu(self.conv2(x)))  # 20Ã—12Ã—12 â†’ 40Ã—4Ã—4
        
        # Flatten for FC layers
        x = x.view(-1, 40 * 4 * 4)             # Flatten
        
        # FC layers with dropout
        x = F.relu(self.fc1(x))               # 640 â†’ 100
        x = self.dropout(x)                    # Dropout
        x = self.fc2(x)                        # 100 â†’ 10
        
        return F.log_softmax(x, dim=1)        # Softmax output
```

---

## ğŸ”¬ Exploration Scripts

| Script | Description |
|--------|-------------|
| `src/level4/1_cnn_pytorch.py` | Modern PyTorch CNN implementation |
| `src/level4/2_visualize_features.py` | Visualize learned filters |
| `src/level4/3_interactive_cnn.py` | Interactive demo with trained CNN |

### Running the Scripts

```bash
cd /home/alcidesticlla/Documents/MOOC/mniels/neural-networks-and-deep-learning/src
python level4/1_cnn_pytorch.py
```

---

## ğŸ“ Key Takeaways

1. **CNNs exploit spatial structure** - pixels near each other are related
2. **Weight sharing reduces parameters** - same filter everywhere
3. **Pooling adds invariance** - small shifts don't matter
4. **ReLU prevents vanishing gradients** - better than sigmoid for deep nets
5. **Dropout prevents overfitting** - like training many networks
6. **Softmax gives probabilities** - outputs sum to 1

---

*Next: Try the interactive CNN demo to draw digits and see real-time predictions!*

