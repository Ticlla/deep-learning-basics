"""
Level 3: Improved Neural Network Techniques
============================================

This script explores the improvements in network2.py:
1. Cross-Entropy Cost Function (faster learning)
2. L2 Regularization (prevent overfitting)
3. Better Weight Initialization (avoid vanishing gradients)

Run from src/ directory:
    python level3/1_improved_network.py

Expected improvement: 94.5% â†’ ~97-98% accuracy
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
import matplotlib.pyplot as plt
import mnist_loader
import network   # Basic network (Level 2)
import network2  # Improved network (Level 3)
from utils import level3_picture, reset_run_timestamp


def section_header(title: str) -> None:
    """Print a formatted section header."""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70 + "\n")


# =============================================================================
# SECTION 1: Cross-Entropy Cost Function
# =============================================================================

def explain_cross_entropy():
    """
    Explain why cross-entropy is better than quadratic cost.
    """
    section_header("SECTION 1: Cross-Entropy Cost Function")
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                 THE LEARNING SLOWDOWN PROBLEM                       â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   With QUADRATIC COST, the gradient contains Ïƒ'(z):                â•‘
    â•‘                                                                     â•‘
    â•‘   âˆ‚C/âˆ‚w = (a - y) Ã— Ïƒ'(z) Ã— input                                  â•‘
    â•‘                      â†‘                                              â•‘
    â•‘                  PROBLEM!                                           â•‘
    â•‘                                                                     â•‘
    â•‘   When Ïƒ(z) â‰ˆ 0 or Ïƒ(z) â‰ˆ 1:                                       â•‘
    â•‘   â€¢ Ïƒ'(z) becomes very small (near 0)                              â•‘
    â•‘   â€¢ Gradient becomes tiny                                          â•‘
    â•‘   â€¢ Learning SLOWS DOWN when predictions are very wrong!           â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    CROSS-ENTROPY SOLUTION                           â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   Cross-Entropy Cost:                                               â•‘
    â•‘   C = -1/n Ã— Î£ [yÂ·ln(a) + (1-y)Â·ln(1-a)]                           â•‘
    â•‘                                                                     â•‘
    â•‘   Gradient (magic happens!):                                        â•‘
    â•‘   âˆ‚C/âˆ‚w = (a - y) Ã— input                                          â•‘
    â•‘           â†‘                                                         â•‘
    â•‘        NO Ïƒ'(z)!                                                    â•‘
    â•‘                                                                     â•‘
    â•‘   Benefits:                                                         â•‘
    â•‘   â€¢ Larger error â†’ Larger gradient â†’ Faster learning               â•‘
    â•‘   â€¢ No slowdown when predictions are very wrong                    â•‘
    â•‘   â€¢ Network learns faster from mistakes                            â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Visualize sigmoid derivative problem
    z = np.linspace(-6, 6, 100)
    sigmoid = 1 / (1 + np.exp(-z))
    sigmoid_prime = sigmoid * (1 - sigmoid)
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Plot 1: Sigmoid and its derivative
    ax1 = axes[0]
    ax1.plot(z, sigmoid, 'b-', linewidth=2, label='Ïƒ(z)')
    ax1.plot(z, sigmoid_prime, 'r-', linewidth=2, label="Ïƒ'(z)")
    ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.5)
    ax1.axhline(y=0.25, color='r', linestyle=':', alpha=0.5, label="max Ïƒ'(z) = 0.25")
    ax1.fill_between(z, 0, sigmoid_prime, alpha=0.2, color='red')
    ax1.set_xlabel('z (weighted input)', fontsize=12)
    ax1.set_ylabel('Value', fontsize=12)
    ax1.set_title("The Vanishing Gradient Problem\nÏƒ'(z) is very small at extremes", fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Annotate problem areas
    ax1.annotate('Ïƒ\'(z) â‰ˆ 0\nLearning slow!', xy=(-4, 0.02), fontsize=10,
                ha='center', color='red')
    ax1.annotate('Ïƒ\'(z) â‰ˆ 0\nLearning slow!', xy=(4, 0.02), fontsize=10,
                ha='center', color='red')
    
    # Plot 2: Compare gradients
    ax2 = axes[1]
    
    # For a wrong prediction where target y=1 but output a is varying
    a_values = np.linspace(0.01, 0.99, 100)
    y = 1  # Target is 1
    
    # Quadratic cost gradient (contains Ïƒ' which we approximate)
    # The gradient magnitude is proportional to |a - y| * Ïƒ'
    z_approx = np.log(a_values / (1 - a_values))  # Inverse sigmoid
    sigma_prime = a_values * (1 - a_values)
    quadratic_grad = np.abs(a_values - y) * sigma_prime
    
    # Cross-entropy gradient is just |a - y|
    cross_entropy_grad = np.abs(a_values - y)
    
    ax2.plot(a_values, quadratic_grad, 'b-', linewidth=2, label='Quadratic: |a-y| Ã— Ïƒ\'(z)')
    ax2.plot(a_values, cross_entropy_grad, 'g-', linewidth=2, label='Cross-Entropy: |a-y|')
    ax2.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5)
    ax2.set_xlabel('Network output a (target y=1)', fontsize=12)
    ax2.set_ylabel('Gradient magnitude', fontsize=12)
    ax2.set_title("Gradient Comparison (target y=1)\nCross-entropy learns faster when wrong!", fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Highlight the key difference
    ax2.annotate('Wrong prediction\n(a â‰ˆ 0)\nQuadratic: slow\nCE: fast!', 
                xy=(0.1, 0.7), fontsize=10, ha='center',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
    
    plt.tight_layout()
    fig_path = level3_picture("cross_entropy_advantage")
    plt.savefig(fig_path, dpi=150)
    plt.close()
    
    print(f"âœ“ Saved: {fig_path}")


def compare_cost_functions():
    """
    Train networks with quadratic vs cross-entropy cost and compare.
    """
    section_header("EXPERIMENT: Quadratic vs Cross-Entropy Cost")
    
    print("Loading MNIST data...")
    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
    
    # Reduce training data for faster comparison
    training_subset = training_data[:10000]
    
    print(f"\nUsing {len(training_subset)} training examples for comparison")
    print("Training two networks for 10 epochs each...\n")
    
    epochs = 10
    
    # Network with Quadratic Cost (old way)
    print("--- Training with QUADRATIC COST ---")
    net_quadratic = network2.Network([784, 30, 10], cost=network2.QuadraticCost)
    _, quad_acc, _, _ = net_quadratic.SGD(
        training_subset, epochs, 10, 0.5,
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True
    )
    
    print("\n--- Training with CROSS-ENTROPY COST ---")
    net_cross_entropy = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
    _, ce_acc, _, _ = net_cross_entropy.SGD(
        training_subset, epochs, 10, 0.5,
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True
    )
    
    # Plot comparison
    fig, ax = plt.subplots(figsize=(10, 6))
    
    epochs_range = range(1, epochs + 1)
    ax.plot(epochs_range, [a/10000*100 for a in quad_acc], 'b-o', 
            linewidth=2, markersize=8, label='Quadratic Cost')
    ax.plot(epochs_range, [a/10000*100 for a in ce_acc], 'g-s', 
            linewidth=2, markersize=8, label='Cross-Entropy Cost')
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
    ax.set_title('Cost Function Comparison\n(10,000 training examples, same architecture)', fontsize=14)
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.3)
    ax.set_xticks(epochs_range)
    
    # Add final accuracy annotations
    ax.annotate(f'{quad_acc[-1]/100:.1f}%', xy=(epochs, quad_acc[-1]/100), 
               xytext=(5, 0), textcoords='offset points', fontsize=10, color='blue')
    ax.annotate(f'{ce_acc[-1]/100:.1f}%', xy=(epochs, ce_acc[-1]/100),
               xytext=(5, 0), textcoords='offset points', fontsize=10, color='green')
    
    plt.tight_layout()
    fig_path = level3_picture("cost_comparison")
    plt.savefig(fig_path, dpi=150)
    plt.close()
    
    print(f"\nâœ“ Saved: {fig_path}")
    print(f"\nResults after {epochs} epochs:")
    print(f"  Quadratic Cost:     {quad_acc[-1]/100:.2f}% accuracy")
    print(f"  Cross-Entropy Cost: {ce_acc[-1]/100:.2f}% accuracy")
    
    return quad_acc, ce_acc


# =============================================================================
# SECTION 2: L2 Regularization
# =============================================================================

def explain_regularization():
    """
    Explain L2 regularization and overfitting.
    """
    section_header("SECTION 2: L2 Regularization (Weight Decay)")
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                     THE OVERFITTING PROBLEM                         â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   Overfitting = Network memorizes training data instead of         â•‘
    â•‘                 learning general patterns                           â•‘
    â•‘                                                                     â•‘
    â•‘   Symptoms:                                                         â•‘
    â•‘   â€¢ Training accuracy: 99%+ (too good!)                            â•‘
    â•‘   â€¢ Test accuracy: 95% (much worse)                                â•‘
    â•‘   â€¢ Network doesn't generalize to new data                         â•‘
    â•‘                                                                     â•‘
    â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
    â•‘   â”‚                                                              â”‚ â•‘
    â•‘   â”‚  Training    â—â—â—â—â—â—â—â—â—â—   (memorized)                       â”‚ â•‘
    â•‘   â”‚  Test        â—â—â—â—â—â—‹â—‹â—‹â—‹â—‹   (can't generalize)                â”‚ â•‘
    â•‘   â”‚                                                              â”‚ â•‘
    â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    L2 REGULARIZATION SOLUTION                       â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   Modified cost function:                                           â•‘
    â•‘                                                                     â•‘
    â•‘   C = Câ‚€ + (Î»/2n) Ã— Î£ wÂ²                                           â•‘
    â•‘        â†‘       â†‘                                                    â•‘
    â•‘   original  penalty for                                             â•‘
    â•‘    cost     large weights                                           â•‘
    â•‘                                                                     â•‘
    â•‘   Effect:                                                           â•‘
    â•‘   â€¢ Penalizes large weights                                        â•‘
    â•‘   â€¢ Forces network to use smaller, distributed weights             â•‘
    â•‘   â€¢ Prevents any single weight from dominating                     â•‘
    â•‘   â€¢ Results in smoother, more general solutions                    â•‘
    â•‘                                                                     â•‘
    â•‘   Weight update becomes:                                            â•‘
    â•‘   w â†’ (1 - Î·Ã—Î»/n) Ã— w - Î· Ã— âˆ‡Câ‚€                                    â•‘
    â•‘       â†‘                                                             â•‘
    â•‘   "weight decay" - weights shrink toward 0                          â•‘
    â•‘                                                                     â•‘
    â•‘   Î» (lambda) controls regularization strength:                      â•‘
    â•‘   â€¢ Î» = 0: no regularization                                       â•‘
    â•‘   â€¢ Î» = 0.1-1: mild regularization                                 â•‘
    â•‘   â€¢ Î» = 5-10: strong regularization                                â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def demonstrate_overfitting():
    """
    Show overfitting by training on small dataset, then fix with regularization.
    """
    section_header("EXPERIMENT: Overfitting and Regularization")
    
    print("Loading MNIST data...")
    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
    
    # Use small training set to induce overfitting
    small_training = training_data[:1000]
    
    print(f"\nUsing only {len(small_training)} training examples (to induce overfitting)")
    print("Training for 50 epochs...\n")
    
    epochs = 50  # Reduced for faster demo
    
    # Without regularization (will overfit)
    print("--- Training WITHOUT regularization (Î»=0) ---")
    net_no_reg = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
    _, val_acc_no_reg, _, train_acc_no_reg = net_no_reg.SGD(
        small_training, epochs, 10, 0.5,
        lmbda=0.0,  # No regularization
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True,
        monitor_training_accuracy=True
    )
    
    print("\n--- Training WITH regularization (Î»=5.0) ---")
    net_reg = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
    _, val_acc_reg, _, train_acc_reg = net_reg.SGD(
        small_training, epochs, 10, 0.5,
        lmbda=5.0,  # With regularization
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True,
        monitor_training_accuracy=True
    )
    
    # Plot comparison
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs_range = range(1, epochs + 1)
    
    # Left plot: Without regularization (overfitting)
    ax1 = axes[0]
    ax1.plot(epochs_range, [a/1000*100 for a in train_acc_no_reg], 'b-', 
            linewidth=2, label='Training (1000 samples)', alpha=0.7)
    ax1.plot(epochs_range, [a/10000*100 for a in val_acc_no_reg], 'r-', 
            linewidth=2, label='Validation (10000 samples)')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Accuracy (%)', fontsize=12)
    ax1.set_title('WITHOUT Regularization (Î»=0)\nOverfitting: Training >> Validation', fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Highlight the gap
    gap_no_reg = train_acc_no_reg[-1]/10 - val_acc_no_reg[-1]/100
    ax1.annotate(f'Gap: {gap_no_reg:.1f}%', xy=(epochs*0.7, 85), fontsize=12,
                bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))
    
    # Right plot: With regularization
    ax2 = axes[1]
    ax2.plot(epochs_range, [a/1000*100 for a in train_acc_reg], 'b-', 
            linewidth=2, label='Training (1000 samples)', alpha=0.7)
    ax2.plot(epochs_range, [a/10000*100 for a in val_acc_reg], 'g-', 
            linewidth=2, label='Validation (10000 samples)')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('WITH Regularization (Î»=5.0)\nSmaller gap = Better generalization', fontsize=12)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    gap_reg = train_acc_reg[-1]/10 - val_acc_reg[-1]/100
    ax2.annotate(f'Gap: {gap_reg:.1f}%', xy=(epochs*0.7, 85), fontsize=12,
                bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))
    
    plt.tight_layout()
    fig_path = level3_picture("regularization_effect")
    plt.savefig(fig_path, dpi=150)
    plt.close()
    
    print(f"\nâœ“ Saved: {fig_path}")
    print(f"\nResults after {epochs} epochs:")
    print(f"  WITHOUT regularization:")
    print(f"    Training:   {train_acc_no_reg[-1]/10:.1f}%")
    print(f"    Validation: {val_acc_no_reg[-1]/100:.1f}%")
    print(f"    Gap: {gap_no_reg:.1f}% (OVERFITTING)")
    print(f"  WITH regularization (Î»=5.0):")
    print(f"    Training:   {train_acc_reg[-1]/10:.1f}%")
    print(f"    Validation: {val_acc_reg[-1]/100:.1f}%")
    print(f"    Gap: {gap_reg:.1f}% (better generalization)")


# =============================================================================
# SECTION 3: Better Weight Initialization
# =============================================================================

def explain_weight_initialization():
    """
    Explain why weight initialization matters.
    """
    section_header("SECTION 3: Better Weight Initialization")
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                  THE SATURATION PROBLEM                             â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   With 784 inputs and standard normal weights:                      â•‘
    â•‘                                                                     â•‘
    â•‘   z = Î£(w_i Ã— x_i) + b                                             â•‘
    â•‘                                                                     â•‘
    â•‘   If weights ~ N(0, 1), then z has std â‰ˆ âˆš784 â‰ˆ 28                 â•‘
    â•‘                                                                     â•‘
    â•‘   Result: |z| is often very large (10-30)                          â•‘
    â•‘           Ïƒ(z) â‰ˆ 0 or 1 (saturated)                                â•‘
    â•‘           Ïƒ'(z) â‰ˆ 0 (learning frozen!)                             â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    THE SOLUTION: 1/âˆšn SCALING                       â•‘
    â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
    â•‘                                                                     â•‘
    â•‘   Initialize weights with:                                          â•‘
    â•‘                                                                     â•‘
    â•‘   w ~ N(0, 1/âˆšn_in)  where n_in = number of inputs                 â•‘
    â•‘                                                                     â•‘
    â•‘   For 784 inputs: w ~ N(0, 1/âˆš784) = N(0, 1/28)                    â•‘
    â•‘                                                                     â•‘
    â•‘   Now z has std â‰ˆ 1, and neurons stay in the                       â•‘
    â•‘   "active" region of the sigmoid where Ïƒ'(z) is larger.            â•‘
    â•‘                                                                     â•‘
    â•‘   Code comparison:                                                  â•‘
    â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â•‘
    â•‘   # Old (network.py):                                               â•‘
    â•‘   weights = np.random.randn(y, x)                                  â•‘
    â•‘                                                                     â•‘
    â•‘   # New (network2.py):                                              â•‘
    â•‘   weights = np.random.randn(y, x) / np.sqrt(x)                     â•‘
    â•‘                                                                     â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)


def compare_initializations():
    """
    Compare old vs new weight initialization.
    """
    section_header("EXPERIMENT: Weight Initialization Comparison")
    
    print("Loading MNIST data...")
    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
    
    print("\nComparing initialization methods on full training set...")
    print("Training for 10 epochs each...\n")
    
    epochs = 10  # Reduced for faster demo
    
    # Old initialization (large weights)
    print("--- Training with LARGE weights (old method) ---")
    net_large = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
    net_large.large_weight_initializer()  # Use old method
    _, large_acc, _, _ = net_large.SGD(
        training_data, epochs, 10, 0.5,
        lmbda=5.0,
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True
    )
    
    print("\n--- Training with 1/âˆšn weights (new method) ---")
    net_small = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)
    # Uses default_weight_initializer automatically
    _, small_acc, _, _ = net_small.SGD(
        training_data, epochs, 10, 0.5,
        lmbda=5.0,
        evaluation_data=validation_data,
        monitor_evaluation_accuracy=True
    )
    
    # Plot comparison
    fig, ax = plt.subplots(figsize=(10, 6))
    
    epochs_range = range(1, epochs + 1)
    ax.plot(epochs_range, [a/10000*100 for a in large_acc], 'r-o', 
            linewidth=2, markersize=6, label='Large weights (std=1)', alpha=0.8)
    ax.plot(epochs_range, [a/10000*100 for a in small_acc], 'g-s', 
            linewidth=2, markersize=6, label='Small weights (std=1/âˆšn)', alpha=0.8)
    
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Validation Accuracy (%)', fontsize=12)
    ax.set_title('Weight Initialization Comparison\n(50,000 training examples)', fontsize=14)
    ax.legend(fontsize=12)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    fig_path = level3_picture("initialization_comparison")
    plt.savefig(fig_path, dpi=150)
    plt.close()
    
    print(f"\nâœ“ Saved: {fig_path}")
    print(f"\nResults after {epochs} epochs:")
    print(f"  Large weights (std=1):   {large_acc[-1]/100:.2f}%")
    print(f"  Small weights (std=1/âˆšn): {small_acc[-1]/100:.2f}%")
    
    return large_acc, small_acc


# =============================================================================
# SECTION 4: Full Training with All Improvements
# =============================================================================

def train_improved_network():
    """
    Train the best network with all improvements.
    """
    section_header("SECTION 4: Full Training with All Improvements")
    
    print("Loading MNIST data...")
    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()
    
    print("""
    Training with ALL improvements:
    âœ“ Cross-Entropy Cost
    âœ“ L2 Regularization (Î»=5.0)
    âœ“ Better weight initialization (1/âˆšn)
    
    Network architecture: [784, 100, 10] (larger hidden layer)
    """)
    
    epochs = 15  # Reduced for faster demo
    
    net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)
    eval_cost, eval_acc, train_cost, train_acc = net.SGD(
        training_data, epochs, 10, 0.5,
        lmbda=5.0,
        evaluation_data=test_data,
        monitor_evaluation_accuracy=True,
        monitor_evaluation_cost=True,
        monitor_training_accuracy=True,
        monitor_training_cost=True
    )
    
    # Plot training progress
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs_range = range(1, epochs + 1)
    
    # Accuracy plot
    ax1 = axes[0]
    ax1.plot(epochs_range, [a/50000*100 for a in train_acc], 'b-', 
            linewidth=2, label='Training', alpha=0.7)
    ax1.plot(epochs_range, [a/10000*100 for a in eval_acc], 'g-', 
            linewidth=2, label='Test')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Accuracy (%)', fontsize=12)
    ax1.set_title('Training Progress - Accuracy', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Cost plot
    ax2 = axes[1]
    ax2.plot(epochs_range, train_cost, 'b-', linewidth=2, label='Training', alpha=0.7)
    ax2.plot(epochs_range, eval_cost, 'g-', linewidth=2, label='Test')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Cost', fontsize=12)
    ax2.set_title('Training Progress - Cost', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    fig_path = level3_picture("full_training")
    plt.savefig(fig_path, dpi=150)
    plt.close()
    
    print(f"\nâœ“ Saved: {fig_path}")
    
    final_accuracy = eval_acc[-1] / 10000 * 100
    print(f"\nğŸ¯ FINAL TEST ACCURACY: {final_accuracy:.2f}%")
    print(f"   (vs ~94.5% from basic network in Level 2)")
    
    return net, eval_acc


def main():
    # Start a fresh run
    reset_run_timestamp()
    
    print("\n" + "â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 10 + "LEVEL 3: IMPROVED NEURAL NETWORK" + " " * 24 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    
    # Section 1: Cross-Entropy
    explain_cross_entropy()
    compare_cost_functions()
    
    # Section 2: Regularization
    explain_regularization()
    demonstrate_overfitting()
    
    # Section 3: Weight Initialization
    explain_weight_initialization()
    compare_initializations()
    
    # Section 4: Full Training
    net, accuracy = train_improved_network()
    
    print("\n" + "=" * 70)
    print("  LEVEL 3 COMPLETE!")
    print("=" * 70)
    print("""
    You've learned:
    
    âœ… Cross-Entropy Cost
       - No Ïƒ'(z) in gradient â†’ faster learning when wrong
    
    âœ… L2 Regularization  
       - Penalizes large weights â†’ prevents overfitting
    
    âœ… Better Initialization
       - Weights ~ N(0, 1/âˆšn) â†’ avoids saturation
    
    ğŸ“ˆ Result: ~97-98% accuracy (vs 94.5% from Level 2)
    
    â†’ Ready for Level 4: Convolutional Neural Networks! ğŸš€
       (Get to 99%+ accuracy)
    """)


if __name__ == "__main__":
    main()

